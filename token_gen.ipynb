{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monet_utils import *\n",
    "import torch\n",
    "from token_generator import token_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Space \n",
    "#----------------------------------------------------------------------------------------------------------------------------------\n",
    "#Backbone \n",
    "patch_size = 16\n",
    "arch = 'dino_small'\n",
    "scale = [patch_size, patch_size]\n",
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "#----------------------------------------------------------------------------------------------------------------------------------\n",
    "#Dataset param\n",
    "data = 'COCO20k'\n",
    "data_set = 'train'\n",
    "remove_hard = True\n",
    "masking_ratio = None\n",
    "masking_size = None\n",
    "resize = None\n",
    "crop = None\n",
    "if data == 'COCO20k':\n",
    "    root_path = './datasets/COCO/images/train2014'\n",
    "elif data == 'VOC07':\n",
    "    root_path = './datasets/VOC2007/VOCdevkit/VOC2007/JPEGImages'\n",
    "elif data == 'VOC12':\n",
    "    root_path = './datasets/VOC2012/VOCdevkit/VOC2012/JPEGImages'\n",
    "#----------------------------------------------------------------------------------------------------------------------------------\n",
    "#Kmeans param\n",
    "k_cluster = 2\n",
    "which_feature = 'output'\n",
    "depth = -1 \n",
    "random_seed = 100\n",
    "n_iter = 200\n",
    "cosine = True\n",
    "if cosine:\n",
    "    spherical = 'cosine'\n",
    "else:\n",
    "    spherical = 'l2'\n",
    "#----------------------------------------------------------------------------------------------------------------------------------\n",
    "#Batch Token Sep\n",
    "#COCO20k는 2개의 파트로 나눔 \n",
    "part = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7309cd89354f3d9b3774eda0f13aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/82.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights founde at dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth and loaded with msg : <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model = get_model(arch=arch, patch_size=patch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.99s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset, dataloader = dataset_load(dataset_name=data, \n",
    "                                   dataset_set=data_set,\n",
    "                                   remove_hards=False, \n",
    "                                   resize = (640,920),\n",
    "                                   patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part number : 1\n",
      "0 | 9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9909/9909 [11:22<00:00, 14.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#Mini Batch Token Gen\n",
    "data_len = len(dataloader)\n",
    "part_num = 2\n",
    "batch_size = data_len//part_num\n",
    "for part in range(1,part_num + 1):\n",
    "    print(f'part number : {part}')\n",
    "    if part == 1:\n",
    "        start_num = 0\n",
    "        end_num = start_num + batch_size\n",
    "    elif part != 1 and part != part_num:\n",
    "        start_num = end_num\n",
    "        end_num = start_num + batch_size\n",
    "    elif part == part_num:\n",
    "        start_num = end_num\n",
    "        end_num = data_len - 1\n",
    "    print(f\"{start_num} | {end_num}\")\n",
    "    token = token_gen(model=model,\n",
    "                      dataset = dataset,\n",
    "                      dataloader=dataloader,\n",
    "                      root_path=root_path,\n",
    "                      patch_size=patch_size,\n",
    "                      start_num=start_num, end_num=end_num,\n",
    "                      depth=-1, device=device)\n",
    "    \n",
    "    np.save(f'/data_hdd1/batch_token_{data}_{patch_size}_{arch}_{which_feature}_{part}_resize640',token)\n",
    "    token = None\n",
    "    if part == 1:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9908 | 19816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9909 [00:00<?, ?it/s]/root/anaconda3/envs/monet_faiss/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/monet_faiss/lib/python3.8/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "100%|██████████| 9909/9909 [29:39<00:00,  5.57it/s]  \n"
     ]
    }
   ],
   "source": [
    "#Part 1만 성공하고 2는 실패했을때\n",
    "#Mini Batch Token Gen\n",
    "data_len = len(dataloader)\n",
    "part_num = 2\n",
    "batch_size = data_len//part_num\n",
    "for part in range(2,part_num + 1):\n",
    "    start_num = 9908\n",
    "    end_num = 19816\n",
    "    print(f\"{start_num} | {end_num}\")\n",
    "    token = token_gen(model=model,\n",
    "                      dataset = dataset,\n",
    "                      dataloader=dataloader,\n",
    "                      root_path=root_path,\n",
    "                      patch_size=patch_size,\n",
    "                      start_num=start_num, end_num=end_num,\n",
    "                      depth=-1, device=device)\n",
    "    \n",
    "    np.save(f'/data_hdd1/batch_token_{data}_{patch_size}_{arch}_{which_feature}_{part}',token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('monet_faiss')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "594829c9a58650178204aba1b35247961316650bd58720877340f655aa425bf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
